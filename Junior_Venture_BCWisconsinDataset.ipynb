{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCc63x8Xe18hVR4imhXL8K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LolaLS/My_Junior_Venture/blob/main/Junior_Venture_BCWisconsinDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **JUNIOR VENTURE PROJECT**\n",
        "\n",
        "Project Notes:\n",
        "\n",
        "*   Since the RSNA Kaggle dataset was too large, I switched to using a different dataset that was uploaded by the University of California, Irvine.\n",
        "*   This new dataset is much smaller and includes features that have already been extracted from mammogram screenings rather than the images themselves.\n",
        "\n",
        "Citations:\n",
        "\n",
        "\n",
        "*   Specific data sources and help is included in comment below.\n",
        "*   Coding help was sourced from Kaggle Learn.\n",
        "*   Code explanations were sourced from Chat GPT.   \n",
        "*   I also used online documentation for specific guidance while working with libraries.\n",
        "\n",
        "Updates:\n",
        "\n",
        "\n",
        "*   17-18/12/2023: So far, I have loaded in all of the data, performed various visualizations, and created a couple very basic models (Decision Tree Regressor and Random Forests Regressor).\n",
        "*  21/12/2023: Changed the n_estimators parameter for the random forest to optimize the model based on trial and error. Checked for missing values and categorical variables. Used label encoder to convert categorical variables. Tried implementing a pipeline, but it wasn't working.\n",
        "*  22/12/2023: Implemented a working pipeline using the same random forests model as before.\n",
        "\n",
        "Next Steps:\n",
        "\n",
        "\n",
        "*   Implement grid search.\n",
        "*   Implement cross validation.\n",
        "*   Use more advanced models and compare outcomes.\n",
        "*   Potentially look at other users models or my own and see how specific parts of the dataset may not be well diagnosed. Attempt to create a model that specifically targets these flaws.\n",
        "*   Try to obtain image data and create new models.\n",
        "\n"
      ],
      "metadata": {
        "id": "_An4B61wGSIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle\n",
        "\n",
        "# SETUP HELP: https://www.youtube.com/watch?v=98xlJvuLMtI"
      ],
      "metadata": {
        "id": "s-VxkLf6s6As"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "zji1eVCvs-Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "tFjmW_KntA2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "abCDHlcotB-E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "GS9AuSoDtDTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d uciml/breast-cancer-wisconsin-data --force\n",
        "\n",
        "# ORIGINAL DATA SOURCE: https://data.world/health/breast-cancer-wisconsin\n",
        "#                       https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic\n",
        "# KAGGLE DATA SOURCE: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data (original upload by UCI Machine Learning)"
      ],
      "metadata": {
        "id": "_cx4wHIbtEwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip breast-cancer-wisconsin-data"
      ],
      "metadata": {
        "id": "Cu9mOOVAtXRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All of the necessary imports.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "8FGb76C7vGlL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists(\"/content/breast-cancer-wisconsin-data.zip\"):\n",
        "    os.symlink(\"data.csv\")"
      ],
      "metadata": {
        "id": "SP6dfkPQvCQ5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_path = 'data.csv'\n",
        "bc_data = pd.read_csv(train_file_path)"
      ],
      "metadata": {
        "id": "1MBYjH4KwK1m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bc_data.head(5) # Visualizing the first 5 rows of the dataset."
      ],
      "metadata": {
        "id": "OvnVdwjSwSi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bc_data.columns # Printing the names of all of the columns of the dataset."
      ],
      "metadata": {
        "id": "lFWcrIb6SWN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values. There are none except 'Unnamed: 32'.\n",
        "\n",
        "missing_columns = [col for col in bc_data.columns if bc_data[col].isnull().any()] # Note that the .isnull() function checks if values are missing while the .any() function checks whether there are any values that satisfy the condition.\n",
        "print (missing_columns)"
      ],
      "metadata": {
        "id": "Y7UWaAoDRozC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for categorical variables. The only one is 'diagnosis', which makes sense because this uses 'M' and 'B' to identify malignant and benign cases. (Help from Chat GPT to break code down).\n",
        "\n",
        "finding_cat_col = (bc_data.dtypes == 'object') # Finding the columns that have categorical values (M and B for diagnosis). In these cases, the data type of the column is 'object'. This is known as a boolean mask and will return true or false values for each column, depending on the type of the entries.\n",
        "categorical_columns = list(finding_cat_col[finding_cat_col].index) # The finding_cat_col[finding_cat_col] line gives the columns that result in the previous statement being true.\n",
        "print (categorical_columns) # Prints the name of the columns at the above indeces."
      ],
      "metadata": {
        "id": "Nedc_KmVRoBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an encoder to transform categorical values to numbers.\n",
        "\n",
        "label_encoder = LabelEncoder() # Define the encoder.\n",
        "label_columns = pd.DataFrame(label_encoder.fit_transform(bc_data[categorical_columns]), columns = ['diagnosis']) # Transform the categorical columns to numerical values using the encoder. Make sure that this is a dataframe with a column name of 'diagnosis' so that it can be concatenated with the other dataframe.\n",
        "\n",
        "missing_y_bc_data = bc_data.drop(categorical_columns, axis = 1) # Drop the original categorical columns from the dataset. When axis = 1, we are dropping columns. When axis = 0, we are dropping rows.\n",
        "encoded_y_bc_data = pd.concat([missing_y_bc_data, label_columns], axis=1) # Concatenate the old data minus the categorical columns with the encoded columns. Used Chat GPT to understand the axis.\n",
        "\n",
        "encoded_y_bc_data # This will be a dataframe."
      ],
      "metadata": {
        "id": "AaH-Y97MqzCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is another way to convert the categorical diagnosis values to numbers. Don't need this because I used the label encoder above.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Models require that the y-values be floats (not strings).\n",
        "# This code changes each of the letters to corresponding numbers.\n",
        "\n",
        "for i in range(len(y)):\n",
        "  if y[i] == 'M':\n",
        "    y[i] = 1.0\n",
        "  elif y[i] == 'B':\n",
        "    y[i] = 0.0\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JNeZ7iipCNYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning the diagnosis values to y.\n",
        "\n",
        "original_y = bc_data.diagnosis\n",
        "print(\"Original Diagnosis Type:\", original_y.dtype) # The type used to be an object: M or B.\n",
        "\n",
        "y = encoded_y_bc_data.diagnosis\n",
        "print(\"Encoded Diagnosis Type:\", y.dtype) # The type is now a integer: 1 (M) or 0 (B)."
      ],
      "metadata": {
        "id": "Gmgt1yGsdAV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the number of malignant and benign cases in the original dataset (categorical diagnosis values).\n",
        "\n",
        "M_counter = 0\n",
        "B_counter = 0\n",
        "for i in range(len(original_y)):\n",
        "  if original_y[i] == 'M':\n",
        "    M_counter = M_counter + 1\n",
        "  elif original_y[i] == 'B':\n",
        "    B_counter = B_counter + 1\n",
        "\n",
        "print(M_counter)\n",
        "print(B_counter)"
      ],
      "metadata": {
        "id": "qSD-JTaFBl4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the number of malignant and benign cases in the new dataset (integer diagnosis values). Comparing these values to the values found above to make sure there is consistency.\n",
        "\n",
        "M_counter = 0\n",
        "B_counter = 0\n",
        "for i in range(len(y)):\n",
        "  if y[i] == 1:\n",
        "    M_counter = M_counter + 1\n",
        "  elif y[i] == 0:\n",
        "    B_counter = B_counter + 1\n",
        "\n",
        "print(M_counter)\n",
        "print(B_counter)"
      ],
      "metadata": {
        "id": "o84D6k4wCe2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = pd.DataFrame(y) # Make sure that y is still a dataframe\n",
        "\n",
        "y"
      ],
      "metadata": {
        "id": "G6zTWo8jBN1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the basic features that will be used for model training.\n",
        "\n",
        "bc_features = ['id', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
        "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
        "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']"
      ],
      "metadata": {
        "id": "1OMymNwkdKP8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning these features to x.\n",
        "\n",
        "x = bc_data[bc_features]"
      ],
      "metadata": {
        "id": "orUTnf2xdjV9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preforming the train-test split (80% of the data is for training while the remaining 20% is for validation).\n",
        "# Checking to make sure the sizes of the data is correct.\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state = 0) # Specifying the random_state makes sure that we get the same split each time.\n",
        "print ('Train Shape:', train_x.shape, train_y.shape)\n",
        "print ('Test Shape:', test_x.shape, test_y.shape)"
      ],
      "metadata": {
        "id": "5TV5k78JdvmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data visualizations to develop understanding of how these features may affect the diagnosis.\n",
        "\n",
        "sns.scatterplot(bc_data, x='radius_mean', y='diagnosis')"
      ],
      "metadata": {
        "id": "6SwdlfyjtWKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(bc_data, x='texture_mean', y='diagnosis')"
      ],
      "metadata": {
        "id": "KHDRlAgQ8Ikp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(bc_data, x='perimeter_mean', y='diagnosis', hue = None)"
      ],
      "metadata": {
        "id": "sgHNP2O88U8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(bc_data, x='area_mean', y='diagnosis')"
      ],
      "metadata": {
        "id": "-Nus2A2V8b31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(bc_data, x='smoothness_mean', y='diagnosis')"
      ],
      "metadata": {
        "id": "uX03vTVT82CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(bc_data, x='compactness_mean', y='diagnosis')"
      ],
      "metadata": {
        "id": "PodSS8jq82cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(bc_data, x='concavity_mean', y='diagnosis')"
      ],
      "metadata": {
        "id": "vtOttIgp85Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(bc_data, x='concave points_mean', y='diagnosis')"
      ],
      "metadata": {
        "id": "Pvq813tK9Iry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(bc_data, x='symmetry_mean', y='diagnosis')"
      ],
      "metadata": {
        "id": "ZS8F-XJs9Ja5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(bc_data, x='fractal_dimension_mean', y='diagnosis')"
      ],
      "metadata": {
        "id": "rANP30fC9JrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Very basic decision tree model set up.\n",
        "bc_DT_model = DecisionTreeRegressor(random_state = 1) # Again, setting the random_state to a constant ensures the same results each time.\n",
        "\n",
        "# Fitting the model to the training data.\n",
        "bc_DT_model.fit(train_x, train_y)\n",
        "\n",
        "# Validating the model to observe accuracy. Accuracy ends up being 9.6% MAE.\n",
        "DT_predictions = bc_DT_model.predict(test_x)\n",
        "mean_absolute_error(test_y, DT_predictions)"
      ],
      "metadata": {
        "id": "ZkLpJ-Xe9i1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slightly more advanced model. Performs slightly better with 8.1% MAE (when n_estimators is optimized using trial and error to avoid under/over-fitting).\n",
        "\n",
        "bc_RF_model = RandomForestRegressor(n_estimators = 30, random_state = 1) # n_estimators represents the number of decision trees in the forest.\n",
        "bc_RF_model.fit(train_x, train_y)\n",
        "RF_predictions = bc_RF_model.predict(test_x)\n",
        "mean_absolute_error(test_y, RF_predictions)"
      ],
      "metadata": {
        "id": "IxnrtBEKJ7w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing a pipeline, instead. Chat GPT helped me debug errors here (ended up adding remainder parameter and transforming categorical y-values separately). Same accuracy as the previous random forest as it is the same exact thing.\n",
        "num_features = train_x.columns\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[('num_scale', StandardScaler(), num_features)], remainder = 'passthrough') # The remainder parameter ensures that any columns that are not included in the transformers are passed through without transformation or error. The standard scaler ensures that the values are on the same scale and can be passed through models.\n",
        "bc_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', bc_RF_model)])\n",
        "\n",
        "bc_pipeline.fit(train_x, train_y)\n",
        "pipeline_RF_predictions = bc_pipeline.predict(test_x)\n",
        "mean_absolute_error(test_y, pipeline_RF_predictions)"
      ],
      "metadata": {
        "id": "qPY5lLeQLMLi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}